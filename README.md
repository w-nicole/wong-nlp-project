
# Added text

A copy of the crosslingual-nlp repository, with the following general edits:

- Changes to minor hyperparameters to the corresponding paper at https://arxiv.org/abs/1904.09077l.
    - But NOT changes to other processing or details described in the paper that differ from the original code.
- A different environment was used, so the .yml was deleted.
- Simplified `example/surprising-mbert/evaluate.sh` script due to no need for non-POS evaluation.
- A few scripts and files added for convenience of analyzing and executing various runs (`run_multiple_check_variation.sh`, anything with `download` in its name in `src`, `src/scratchwork`)
- Change to `constant.py` to get labels to run with older UD.
- Changes to the way that types are represented in `util.py` to resolve compile errors/possible library inconsistencies.

Changes are not always marked in individual files, but the above was generated by going through the complete diff as of 3/28/22.

For the state of the repository before changes, see commit c029e10e909039946a7555ddad87d99e9e0f9fc9

For the state of the repository used for replication numbers before UD change, see commit b1a9af51796f62b9436f2860e44b86891b77be5b.

# Crosslingual NLP

This repo supports various cross-lingual transfer learning & multilingual NLP models. It powers the following papars.

- Mahsa Yarmohammadi*, Shijie Wu*, Marc Marone, Haoran Xu, Seth Ebner, Guanghui Qin, Yunmo Chen, Jialiang Guo, Craig Harman, Kenton Murray, Aaron Steven White, Mark Dredze, and Benjamin Van Durme. [*Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction*](https://arxiv.org/abs/2109.06798). EMNLP. 2021. ([Experiments Detail](example/data-projection))
- Shijie Wu and Mark Dredze. [*Do Explicit Alignments Robustly Improve Multilingual Encoders?*](https://arxiv.org/abs/2010.02537) EMNLP. 2020. ([Experiments Detail](example/contrastive-alignment))
- Shijie Wu and Mark Dredze. [*Are All Languages Created Equal in Multilingual BERT?*](https://arxiv.org/abs/2005.09093) RepL4NLP. 2020. ([Experiments Detail](example/low-resource-in-mbert))
- Shijie Wu*, Alexis Conneau*, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. [*Emerging Cross-lingual Structure in Pretrained Language Models*](https://arxiv.org/abs/1911.01464). ACL. 2020. ([Experiments Detail](example/emerging-crossling-struct))
- Shijie Wu and Mark Dredze. [*Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT*](https://arxiv.org/abs/1904.09077). EMNLP. 2019. ([Experiments Detail](example/surprising-mbert))



## Miscellaneous

- Environment (conda): `environment.yml`
- Pre-commit check: `pre-commit run --all-files`

## License

MIT
